# 1. MoAI Platform 병렬화 - Llama3 70B

Llama3 70B를 full fine-tuning을 진행하는데에는 필수적으로 다중 GPU를 사용해야하고, Tensor Parallelism, pipeline parallel, data parallel과 같은 병렬화를 수행해야합니다. 이를 수행하기 위해서는 Deepspeed와 같은 툴을 이용하여 복잡한 config 설정을 해주어야하며, pipeline parallel의 경우 모델을 잘 이해하고 활용할 줄 알아야합니다. 또한 실제로 제일 효과적인 병렬화 방식을 찾기 위해서는 여러 번 코드를 수정하고 모델을 학습시켜보며 최적의 조합을 찾아야합니다. **사용자는 이처럼 다중 GPU를 사용하기 위해선는 많은 노력이 필요합니다.**

하지만 MoAI Platform에서는 위와 같은 **복잡한 코드 수정이나 모델에 대한 깊은 이해 없이, 자동 병렬화와 그 최적의 조합을 제공하여 사용자에게 간단하고도 최고의 모델 학습을 제공**합니다.

## Fine-tuning Code

**MoAI Platform의 모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.** Llama3 70B를 학습하기 위해서는 아래와 같이 기존 pytorch에서 하나의 gpu를 사용하듯이 스크립트를 작성할 수 있습니다.

```python
    torch.moreh.option.enable_advanced_parallelization()
    
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-70B")
		tokenizer = LlamaTokenizer.from_pretrained("meta-llama/Meta-Llama-3-70B")
    
    # Compose pad token mask
    def create_mask(input_ids, tokenizer):
		    pad_token_ids = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
			  return (input_ids != pad_token_ids).long() 
			   
    # Mask pad tokens for training
    def mask_pads(inputs, tokenizer, ignore_index = -100):
        idx_mask = create_mask(inputs, tokenizer)
        labels = copy.deepcopy(inputs)
        labels[~idx_mask.bool()] = ignore_index
        return labels

    # Define AdamW optimizer
    optim = AdamW(model.parameters(), lr=args.lr)

    # Start training
    for epoch in range(args.num_train_epochs):
        for step, batch in enumerate(train_dataloader, start=1):
            #breakpoint()
            start_time = time.perf_counter()
            input_ids = batch["input_ids"]
            inputs, labels = input_ids, mask_pads(input_ids, tokenizer)
            attn_mask = create_mask(inputs, tokenizer)
            outputs = model(
                input_ids.cuda(),
                attention_mask=attn_mask.cuda(),
                labels=labels.cuda(),
                use_cache=False,
            )
            loss = outputs[0]
            loss.backward()

            optim.step()
            model.zero_grad(set_to_none=True)
```

위 스크립트에서 유일하게 기존 naive pytorch와 다르게 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform의 자동 최적화 및 병렬화를 설정하는 코드로 사용자는 간단하게 Llama3 70B를 finetuning할 수 있습니다.

```bash
torch.moreh.option.enable_advanced_parallelization()
```

이제부터 MoAI Platform에서 mi250 64개를 이용한 Llama3 70B 모델 fine-tuning 튜토리얼을 시작하겠습니다. 사용자는 이 튜토리얼을 통해서 MoAI Platform에서 multi-gpu를 사용하는 방법이 얼마나 쉽고 효과적인지 확인할 수 있을 것입니다.