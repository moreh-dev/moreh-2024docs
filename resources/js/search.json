[[{"l":"MoAI Documentation","p":["MoAI(Moreh AI appliance for AI accelerators) Platform is a scalable AI platform that enables easy control of thousands of Graphics Processing Units(GPUs) essential for developing large-scale deep learning models.","Virtual GPU Clusters- Access and utilize virtualized GPU clusters to scale your computational resources seamlessly.","Fine-Tuning- Fine-tune with a few commands and deploy your fine-tuned model for inference.","Advanced Parallelism- Employ advanced parallelism techniques effortlessly to optimize model training."]},{"l":"Getting Started","p":["Get started with fine-tuning MoAI Platform Beginner's Guide for Finetuning","AP Guide Advanced Parallelization (AP) Feature Instructions","Moreh Toolkit Guide Command Line Usage","MoAI Platform Features The virtualization and parallelization features of the MoAI Platform"]},{"i":"what-is-moai-platform","l":"What is MoAI Platform?","p":["MoAI(Moreh AI appliance for AI accelerators) Platform is a scalable AI platform that enables easy control of thousands of Graphics Processing Units(GPUs) essential for developing large-scale deep learning models."]},{"l":"Core Technologies of MoAI Platform","p":["As deep learning models evolve, they become increasingly complex and require substantial computational resources, with parameters expanding from billions to trillions. Developing large-scale models involves managing and processing an immense number of parameters, which is very challenging and time-consuming.","The MoAI Platform's automatic parallelization addresses these challenges by simultaneously processing multiple tasks, determining the optimal calculation method for large models. This allows users to focus solely on their core AI challenges, regardless of their application scale or processor type. Furthermore, it efficiently utilizes GPU computational resources at a reasonable cost by allocating them only during calculation execution.","Various Accelerators, Multi-GPU Support","GPU Virtualization","Dynamic GPU Allocation","AI Compiler Automatic Parallelization"]}],[{"l":"Overview"},{"i":"what-is-moai-platform","l":"What is MoAI Platform?","p":["The MoAI Platform is an AI platform designed for developing large-scale deep learning models, allowing users to easily control thousands of GPU clusters for training or inference of AI models."]},{"l":"Core Technologies of MoAI Platform","p":["As deep learning models evolve, they become more complex, comprising numerous layers with a multitude of parameters. Consequently, large-scale computing resources have become essential components of AI infrastructure. To develop models using such resources, optimizing the training process, including parallelizing the model and manually configuring the cluster environment, is crucial. Managing GPU and node resources for optimal training requires significant time and effort from developers.","To address these challenges, the MoAI Platform offers the following features to support efficient infrastructure in the era of large-scale AI:","Various Accelerators, Multi-GPU Support","GPU Virtualization","Dynamic GPU Allocation","AI Compiler Automatic Parallelization"]},{"i":"1-various-accelerators-multi-gpu-support","l":"1. Various Accelerators, Multi-GPU Support","p":["MoAI Platform supports various AI accelerators, allowing users to execute diverse model training and inference tasks regardless of accelerator type. Users can seamlessly utilize accelerators other than AMD, Intel, and NVIDIA without the need to modify existing training and inference code written in Python."]},{"l":"2. GPU Virtualization","p":["The virtualization feature of the MoAI Platform enables thousands of GPUs to be utilized as a single GPU (Single Virtual Device). This allows AI engineers to easily and quickly execute deep learning training and inference without the need for optimization processes like multi-GPU and multi-node parallelization. By assuming only one GPU and structuring Python code accordingly, users can perform these tasks effortlessly.","Users can expand or shrink GPU resources as needed, increasing the scalability of the service. With a simple one-line command in the MoAI Platform, GPU resources virtualized into a single virtual device can be easily expanded or contracted."]},{"l":"3. Dynamic GPU Allocation","p":["In Public Cloud, billing starts when VM instances are created, and changing GPUs requires recreating the instances. Furthermore, once selected, it can be challenging to flexibly change the chosen virtual machine, which can hinder optimization according to the user's needs.","The MoAI Platform is designed to charge fees on a per-minute basis only when AI accelerators are actually in operation, allowing for a complete pay-as-you-go system. This design enables significant cost savings compared to existing cloud services by freeing GPUs from dependency on specific virtual machines (VMs) according to user usage patterns."]},{"l":"4. AI Compiler Automatic Parallelization","p":["Deep learning models consist of multiple layers, each containing various operations. These operations can be learned independently, enabling parallel processing. However, ML engineers must manually configure combinations of parameters and environment variables for this purpose. The MoAI Platform's automatic parallelization feature swiftly determines the optimal combination of parallelization environment variables. As a result, users can train models automatically applying parallelization techniques such as Data Parallelism, DDP, Pipeline Parallelism, and Tensor Parallelism during large-scale model training.","In the era of artificial intelligence, significant GPU clusters and effective GPU parallelization are required for training and inference of large-scale models such as Large Language Models(LLMs) and Large Multimodal Models(LMMs).","Common AI frameworks used with NVIDIA currently require AI engineers to manually adjust parallelization based on model size, complexity, and available GPU or cluster sizes. This process is time-consuming and often takes several weeks.","The MoAI Platform provides automatic parallelization through the Moreh AI compiler, optimizing GPU resources based on the size of specific AI models and GPU clusters. Through automatic parallelization, model training, which typically takes weeks in an NVIDIA environment, can be dramatically shortened to approximately 2-3 days."]}],[{"l":"MoAI Platform Features","p":["The MoAI Platform handles hundreds of GPUs as a single accelerator through virtualization and reduces the burden of managing complex GPU clusters through parallelization.","Parallelization","GPU Virtualization"]}],[{"i":"gpu-virtualization-moai-accelerator","l":"GPU Virtualization: MoAI Accelerator","p":["The MoAI Platform virtualizes large GPU clusters, consisting of dozens or hundreds of GPU nodes, into a single accelerator called the MoAI Accelerator. This allows users to design and train models as if they are using a single GPU, without worrying about model parallelization or manually configuring cluster environments.","You can check the MoAI Accelerator status by entering the moreh-smi command in the terminal.","The output shows that the user is utilizing a single accelerator with 2048 GB of memory. However, in reality, it consists of 4 nodes, each with 4 GPUs.","Let's verify if the MoAI Accelerator is recognized correctly in PyTorch, one of the most widely used deep learning frameworks. Using the cuda API in the Python interpreter, we can see that PyTorch recognizes the MoAI Accelerator as a single device.","An important point to note is that there are no physical GPUs in the user's environment. When the user attempts to use GPU accelerators through APIs like cuda in deep learning frameworks such as PyTorch, the MoAI Platform automatically allocates GPU cluster resources."]},{"l":"Dynamic GPU Allocation on the MoAI Platform","p":["The MoAI Platform dynamically handles GPU allocation at the process level. This ensures that users efficiently receive physical GPU allocations while training and inferencing models using frameworks like PyTorch. It also provides flexibility, allowing users to select and adjust the number of GPUs from various pre-defined MoAI Accelerator flavors as needed.","In contrast, traditional cloud platforms typically allocate physical GPUs statically from the moment an instance is created. If users wish to change the number of GPUs or stop using them, they need to delete the existing instance or terminate the container and restart it. The MoAI Platform's dynamic allocation significantly reduces this inconvenience.","Let's go through a simple example of changing the MoAI Accelerator flavor.","First, check the current MoAI Accelerator in use by entering the moreh-smi command in the terminal.","You can see that the current flavor of the MoAI Accelerator being used is . If you need to train a larger model or want to use more GPUs to speed up training, you can easily switch the flavor by entering the moreh-switch-model command.","After entering the moreh-smi command again to check the current MoAI Accelerator flavor, you can see it has been successfully changed to ."]},{"l":"Conclusion","p":["The MoAI Platform simplifies the complexity of multi-node GPU clusters through virtualization technology known as the MoAI Accelerator. It provides users with a powerful yet flexible computing environment. By allowing users to adjust model size and the number of GPUs without complex settings and management tasks, it enables efficient resource utilization. Use the MoAI Accelerator on the MoAI Platform to design and train deep learning models quickly and efficiently."]}],[{"i":"automatic-parallelization-ap","l":"Automatic Parallelization: AP","p":["Automatic Parallelization On the MoAI Platform, users interact with the MoAI Accelerator, which is presented as a single virtualized GPU. This means users can write code assuming the use of a single GPU without needing to worry about parallelization. But how does the MoAI Platform handle multiple GPUs?","The MoAI Platform automatically optimizes and parallelizes based on the number of GPUs in use.","For instance, if you start training with an accelerator flavor that uses 8 GPUs, the MoAI Platform automatically divides the total batch size into 8 parts, distributing them across each GPU. Let's take an example of fine-tuning the Llama3-8b model. If you select an accelerator flavor with 4 GPUs and set a batch size of 16, the platform automatically assigns 4 batches per GPU, processing approximately 125,000 tokens per second.","For faster and more efficient training, you can select an accelerator flavor with more GPUs and increase the batch size. If you choose an accelerator flavor with 16 GPUs and set the batch size to 64, the number of tokens processed per second can increase fourfold compared to the previous setup.","But what if you want to use an even larger batch size? Without additional code modifications, a typical GPU cluster might run into Out of Memory (OOM) errors. However, the MoAI Platform can automatically parallelize the model to continue training.","When you choose an accelerator flavor with 16 GPUs and set the batch size to 512, the platform applies model parallelization and data parallelization simultaneously. This allows training with a larger batch size using the same number of GPUs.","Additionally, even large models like the 70B model can be automatically parallelized without any extra work, making training straightforward. The MoAI Platform provides automatic optimization and parallelization based on the model and batch size, enabling convenient and efficient use of multiple GPUs."]}],[{"l":"Fine-tuning Tutorials","p":["This tutorial is for anyone who wants to fine-tune powerful large language models such as Llama2, Mistral, and etc for their own projects. We will walk through the steps to finetune these large language models (LLMs) with MoAI Platform.","Llama2","Llama3 8B","Mistral","GPT","Qwen","Baichuan2","Fine-tuning in machine learning involves adjusting a pre-trained machine learning model's weight on new data to enhance task-specific performance. Essentially, when you want to apply an AI model to a new task, you take an existing model and optimize it with new datasets. This allows you to customize the model to meet your specific needs and domain requirements.","A pre-trained model has a large number of parameters designed for general-purpose use, and effectively fine-tuning such a large model requires a sufficient amount of training data.","With the MoAI Platform, you can easily apply optimized parallelization techniques that consider the GPU's memory size, significantly reducing the time and effort needed before starting training."]},{"i":"what-you-will-learn-here","l":"What you will learn here:","p":["Loading datasets, models, and tokenizers","Running training and checking results","Applying automatic parallelization","Choosing the right training environment and AI accelerators"]}],[{"l":"Llama2 Fine-tuning","p":["This tutorial introduces an example of fine-tuning the open-source Llama2 13B model on the MoAI Platform. Through this tutorial, you'll learn how to leverage the AMD GPU cluster using the MoAI Platform and explore the benefits of performance and automatic parallelization."]},{"l":"Overview","p":["The Llama2 model, released by Meta in July 2023, is an open-source model based on the Decoder-only Transformer. It follows the structure of the existing Llama model but has been trained with 40% more data to understand more diverse and complex information.","Llama2 excels particularly in language understanding and generation tasks, achieving state-of-the-art performance in various natural language processing tasks. This model supports multilingual capabilities, enabling processing of text in various languages worldwide, and is publicly accessible for research and development purposes.","In this tutorial, we will fine-tune the Llama2 model on the MoAI Platform using the CNN Daily Mail dataset focus on summarization task. Summarization is one of the natural language processing techniques, where the task is to unravel long, complex text and deliver precise summaries."]},{"l":"Before You Start","p":["Be sure to obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. You can either request and use a trial container of the MoAI Platform or sign up for the public cloud service running on the MoAI Platform.","Inquiries for MoAI Platform Trial Container: support@moreh.io","KT Cloud Hyperscale AI Computing","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Check MoAI Accelerator","p":["To train models like the Llama2 model outlined in this tutorial, you need to select an appropriate size MoAI Accelerator. Start by using the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Model fine-tuning"]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.2.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to confirm that the torch package is properly imported and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_llama2.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download the Model and Tokenizer","p":["Download the checkpoint and tokenizer for the Llama2-13b-hf model using Hugging Face. Please note that the Llama2 model requires community license agreement and Hugging Face token information. Additionally, since the checkpoint size for the Llama2 13B model is approximately 49GB, it is essential to have at least 50GB of storage space for the checkpoint.","Begin by visiting the following website and providing the required information to proceed with the license agreement.","meta-llama/Llama-2-13b-hf · Hugging Face","Once you've submitted the agreement form, check that the status on the page has updated as follows:","Once the status has changed, you can utilize the download_llama2_13b.py script found in the tutorial directory to download the model checkpoint and tokenizer into the ./llama-2-13b-hf directory.","Make sure to replace user-token with your Hugging Face token.","Check if the model checkpoint and tokenizer have been downloaded."]},{"l":"Download Training Data","p":["To download the training data, we'll use the prepare_llama2_dataset.py script located in the dataset directory. When you run the code, it will download the cnn_dailymail dataset, preprocess it for training, and save it as llama2_dataset.pt file.","You can then load the stored dataset in your code like this:"]}],[{"l":"2. Understanding training code","p":["If you've got all your training data ready, let's dive into running the actual fine-tuning process using the train_llama2.py script. This script is just standard PyTorch code, performing fine-tuning based on the Llama2 13B model from the Hugging Face Transformers library.","We highly recommend proceeding with the tutorial using the provided script as is. Afterward, feel free to customize the script to fine-tune the Llama2 13B model or any other publicly available model in a different manner. If needed, refer to the MoAI Platform application guide LLM Fine-tuning Parameter Guide provided by Moreh."]},{"l":"Training Code","p":["All the code used during training is exactly the same as when you're using PyTorch in general.","Import the necessary modules from the transformers library.","Then, load up the model checkpoint and tokenizer you downloaded earlier.","Load your preprocessed dataset, which you prepared during the 1. Prepare fine-tuning step, and define your data loaders.","Training proceeds as usual, just like with any other PyTorch model.","With MoAI Platform, you can seamlessly use your existing PyTorch scripts without any modifications."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","Training a large language model like Llama2 13B requires a significant amount of GPUs. Without using the MoAI Platform, you would need to implement parallelization techniques such as data parallelism, pipeline parallelism, and tensor parallelism to perform the training.","(Reference: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","MoAI Platform's Advanced Parallelization (AP) provides optimization and automation features that are difficult to experience in other frameworks. With the AP feature, you can easily configure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism which are typically required for large-scale model training, with just a single line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Before continuing with the tutorial, we recommend reaching out to your infrastructure provider to inquire about the types and quantities of GPUs associated with each flavor. Once you have this information, you can choose one of the following flavors to proceed:","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Remember when we checked the MoAI Accelerator in the 1. Prepare Fine-tuning? Now let's set up the accelerator needed for learning.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 512GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the given train_gpt.py script.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","You can verify that the training is proceeding smoothly by checking the training logs.","The throughput displayed during training indicates how many tokens are being trained per second through the PyTorch script.","Throughput when using 16 AMD MI250 GPUs: Approximately 35,000 tokens/sec","Here are the approximate training times based on the type and number of GPUs:","Training time when using 16 AMD MI250 GPUs: Approximately 10 hours"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. Then, execute the moreh-smi command to observe the MoAI Accelerator occupying memory and the training script running. Make sure to check this while the initialization process is completed and the training loss appears in the execution logs."]}],[{"l":"4. Checking Training Results","p":["Upon running the train_llama2.py script as described earlier, the resulting model will be saved in the llama2_summarization directory. This model is a pure PyTorch parameter file and is fully compatible with regular GPU servers, not just the MoAI Platform.","You can test the trained model using the inference_llama2.py script located under the tutorial directory of the pre-downloaded GitHub repository.","For testing, articles related to English Premier League (EPL) match results have been used.","Run the train script.","From the output, you'll notice that Llama2 has appropriately summarized the contents of the prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer again to the 3. Model fine-tuning.","Please contact your infrastructure provider and choose one of the following options before proceeding. KT Hyperscale AI Computing (HAC) AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_llama2.py script again.","Since the GPU memory has doubled, let's increase the batch size from the previous 256 to 512 and run the code again.","If the training proceeds smoothly, you'll see logs similar to the following:","Compared to the previous results obtained when the GPU count was halved, you'll notice that the training is progressing similarly, but with an improved throughput.","When using AMD MI250 GPU 16 → 32 : approximately 35,000 tokens/sec → 74,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["From this tutorial, we have seen how to fine-tune Llama2 13B for text summarization on the MoAI Platform. Open-source LLMs like Llama can be utilized for various natural language processing tasks such as summarization, question answering, and more. With the MoAI Platform, you can easily configure the required number of GPUs without any code modifications.","The availability of large language models like LLaMA 2, fine-tuning techniques, and the MoAI Platform makes it possible for anyone to develop powerful AI applications. So please start repeating the same process outlined here on your own data.","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Llama3 8B Fine-tuning","p":["This tutorial introduces an example of fine-tuning the open-source Llama3-8b model on the MoAI Platform. Through this tutorial, you will learn how to use an AMD GPU cluster with MoAI Platform and understand the benefits of its performance and automatic parallelization."]},{"l":"Overview","p":["The Llama3 model is an open-source, decoder-only Transformer model released by Meta in April 2024. It follows the architecture of previous Llama models but is trained on seven times more data (15T), enabling it to understand more diverse and complex information.","Llama3 excels in tasks involving language understanding and generation, achieving performance that significantly surpasses previous state-of-the-art results in various natural language processing tasks. It supports multiple languages, making it capable of processing texts from around the world, and is widely accessible for research and development purposes.","In this tutorial, we will fine-tune the Llama3 model on the MoAI Platform for a summarization task using the CNN Daily Mail dataset."]},{"l":"Before You Start","p":["Be sure to obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. You can either request and use a trial container of the MoAI Platform or sign up for the public cloud service running on the MoAI Platform.","Inquiries for MoAI Platform Trial Container: support@moreh.io","KT Cloud Hyperscale AI Computing","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Checking the MoAI Accelerator","p":["To train sLLMs like the Llama3 model described in this tutorial, you need to select an appropriately sized MoAI Accelerator. First, use the moreh-smi command to check the current MoAI Accelerator in use.","Detailed instructions on configuring the MoAI Accelerator for your specific training needs will be provided in section \"3. Model fine-tuning\""]}],[{"l":"1. Prepare Fine-tuning","p":["Setting up the PyTorch execution environment on the MoAI Platform is similar to setting it up on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the MoAI version required to run it. In the example above, it indicates that PyTorch 1.13.1+cu116 is installed with MoAI version 24.5.0.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to confirm that the torch package is properly imported and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_llama3.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download the Model and Tokenizer","p":["Use Hugging Face to download the checkpoint and tokenizer for the Llama3-8b model. Note that you will need to agree to the community license and provide your Hugging Face token information. Additionally, for the Llama3 8B model, you should have approximately 20GB of free storage available for the checkpoint, which is around 16GB.","First, enter the required information and agree to the license on the following site.","https://huggingface.co/meta-llama/Meta-Llama-3-8B","After submitting the agreement, confirm that the page status has changed as shown below.","Once the status has changed, you can use the download_llama3_8b.py script in the tutorial directory to download the model checkpoint and tokenizer to the ./llama3-8b directory.","Replace user-token with your Hugging Face token.","Check if the model checkpoint and tokenizer have been downloaded successfully."]},{"l":"Download Training Data","p":["To download the training data, use the prepare_llama3_dataset.py script in the dataset directory. Running this script will download and preprocess the cnn_dailymail dataset for training, saving it as the llama3_dataset.pt file.","You can load and use the saved dataset in your code as follows."]}],[{"l":"2. Understanding training code","p":["Once you have prepared all the training data, let's take a look at the contents of the train_llama3.py script, which will carry out the actual fine-tuning process. This script executes fine-tuning based on the implementation of the Llama3 8B model in the Hugging Face Transformers library, using standard PyTorch code.","We recommend initially proceeding with the provided script as is until the end of the tutorial. Afterwards, feel free to modify the script as desired to fine-tune the Llama3 8B model in different ways. This flexibility is possible due to MoAI Platform's complete compatibility with PyTorch. If needed, refer to the MoAI Platform Application Guide provided by Moreh ( LLM Fine-tuning Parameter Guide)."]},{"l":"Training Code","p":["All the code used during training is exactly the same as when you're using PyTorch in general.","Import the necessary modules from the transformers library.","Then, load up the model checkpoint and tokenizer you downloaded earlier.","Load your preprocessed dataset, which you prepared during the 1. Prepare fine-tuning step, and define your data loaders.","Subsequently, the training proceeds similarly to regular PyTorch model training.","In this way, you can write code in MoAI Platform using the same approach as with standard PyTorch code."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","Training a large language model like Llama2 13B requires a significant amount of GPUs. Without using the MoAI Platform, you would need to implement parallelization techniques such as data parallelism, pipeline parallelism, and tensor parallelism to perform the training.","(Reference: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","MoAI Platform's Advanced Parallelization (AP) provides optimization and automation features that are difficult to experience in other frameworks. With the AP feature, you can easily configure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism which are typically required for large-scale model training, with just a single line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will actually execute the fine-tuning process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Remember when we checked the MoAI Accelerator in the 1. Llama3 8B Fine-tuning? Now let's set up the accelerator needed for learning.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 512GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_llama3.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","Upon checking the training logs, you can confirm that the training is progressing smoothly.","The throughput displayed during training indicates how many tokens per second the script is training through this PyTorch script.","When using AMD MI250 GPU with 16 GPUs: Approximately 200,000 tokens/sec","The approximate training time depending on the GPU type and quantity is as follows:","When using AMD MI250 GPU with 16 GPUs: Approximately 90 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. Then, execute the moreh-smi command to observe the MoAI Accelerator occupying memory and the training script running. Make sure to check this while the initialization process is completed and the training loss appears in the execution logs."]}],[{"l":"4. Checking Training Results","p":["When you execute the train_llama3.py script as in the previous section, the resulting model will be saved in the llama3_summarization directory. This model is compatible with any GPU server, not just MoAI Platform, as it is stored as a pure PyTorch model parameter file.","You can test the trained model using the inference_llama3.py script located under the tutorial directory in the GitHub repository you downloaded earlier.","For testing, articles related to soldiers deployed in Iraq were used.","Run the code.","Upon examining the output, you will see that the model appropriately summarizes the contents of the input prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer again to the 3. Model fine-tuning.","Please contact your infrastructure provider and choose one of the following options before proceeding. KT Hyperscale AI Computing (HAC) AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Again, run the train_llama3.py script.","Since the available GPU memory has doubled, let's increase the batch size from the previous 256 to 512 and run the code again.","If the training proceeds normally, you will see similar logs to the previous run but with improved throughput due to the doubled number of GPUs.","When using AMD MI250 GPU 16 → 32 : From approximately 200,000 tokens/sec to 390,000 tokens/sec."]}],[{"l":"6. Conclusion","p":["From this tutorial, we have seen how to fine-tune Llama3 8B on the MoAI Platform. Open-source LLMs like Llama can be utilized for various tasks such as summarization, question answering, and more. With the MoAI platform, you can easily configure the number of GPUs you need without any code changes.","The availability of large language models like LLaMA 3, fine-tuning techniques, and the MoAI Platform makes it possible for anyone to develop powerful AI applications. So please start developing models by repeating the process outlined here on your own data.","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)"]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Mistral Fine-tuning","p":["This tutorial guides you on fine-tuning the open-source Mistral 7B model on the MoAI Platform. You'll learn to utilize an AMD GPU cluster using the MoAI Platform and experience the improved performance and the benefits of automatic parallelization."]},{"l":"Overview","p":["The Mistral model, released by Mistral AI in 2023, is a giant language model. It has gained attention for outperforming larger models in complex tasks like code generation, question answering, and solving mathematical problems.","The Mistral 7B model uses only the Transformer's decoder, applying techniques like Sliding Window Attention to efficiently process the length of input tokens and introducing Rolling Buffer Cache to optimize memory usage.","In this tutorial, we'll fine-tune the Mistral 7B model using the python_code_instructions_18k-alpaca dataset for the code generation task on the MoAI Platform."]},{"l":"Before You Start","p":["Be sure to obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. You can either request and use a trial container of the MoAI Platform or sign up for the public cloud service running on the MoAI Platform.","Inquiries for MoAI Platform Trial Container: support@moreh.io","KT Cloud Hyperscale AI Computing","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Check MoAI Accelerator","p":["To train models like the Llama2 model outlined in this tutorial, you need to select an appropriate size MoAI Accelerator. Start by using the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Model fine-tuning."]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.2.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_mistral.py script located inside the tutorial directory."]},{"l":"Download the Model and Tokenizer","p":["Let's download the checkpoint and tokenizer for the Mistral 7B v0.1 model using Hugging Face. This process requires agreeing to the community license and providing your Hugging Face token information to access the Mistral model. Additionally, since the checkpoint size for the Mistral 7B model is approximately 15GB, it's essential to have at least 16GB of storage space available to store the checkpoint.","First, enter the required information on the Hugging Face website below and proceed with the license agreement.","mistralai/Mistral-7B-v0.1 · Hugging Face","After submitting the agreement form, confirm that the status on the page has changed as follows:","If the status has been updated, you can use the download_mistral_7b.py script located in the tutorial directory to download the model checkpoint and tokenizer into the ./mistral-7b directory.","Replace user-token with your Hugging Face token.","Check if the model checkpoint and tokenizer have been downloaded."]},{"l":"Download Training Data","p":["In this tutorial, we will use the python_code_instructions_18k_alpaca dataset (11.4 MB) available on Hugging Face among various datasets publicly available for code generation training.","We will execute prepare_mistral_dataset.py to download the dataset and preprocess it for training.","The preprocessed dataset will be saved as mistral_dataset.pt.","You can load the saved dataset in your code as follows."]}],[{"l":"2. Understanding training code","p":["Once you have prepared all the training data, let's delve into the contents of the train_mistral.py script to execute the actual fine-tuning process. In this step, you will confirm MoAI Platform's full compatibility with PyTorch, ensuring that the training code is identical to general PyTorch code for Nvidia GPUs. Moreover, you'll explore how efficiently MoAI Platform implements complex parallelization techniques beyond the conventional scope.","We highly recommend proceeding with the tutorial using the provided script as is. Afterward, feel free to customize the script to fine-tune the Llama2 13B model or any other publicly available model in a different manner. If needed, refer to the MoAI Platform application guide LLM Fine-tuning Parameter Guide provided by Moreh."]},{"l":"Training Code","p":["All the code used during training is exactly the same as when you're using PyTorch in general.","Import the necessary modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the preprocessed dataset saved during the preparation for 1. Prepare Fine-tuning and define the data loaders.","Subsequently, the training proceeds similarly to general AI model training with Pytorch.","With MoAI Platform, you can seamlessly use your existing PyTorch scripts without any modifications."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","For colossal language models like Mistral 7B used in this tutorial, it's imperative to train them using multiple GPUs. When using frameworks other than MoAI Platform, you'll need to introduce parallelization techniques such as Data Parallel, Pipeline Parallel, and Tensor Parallel.","For instance, if a user wants to apply DDP in their typical PyTorch code, they would need to add the following code snippet. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","MoAI Platform's Advanced Parallelization(AP) provides optimization and automation features that are difficult to experience in other frameworks. Through the AP feature, users can experience the best distributed parallel processing. By leveraging AP, users can easily configure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism required for training large-scale models with just a single line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Do you remember checking MoAI Accelerator in the Mistral Fine-tuning document? Now let's set up the accelerator needed for learning.","Enter 8 to use .","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 64GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_mistral.py script below.","If the training proceeds smoothly, you should see the following log. Take note of the sections highlighted in blue, as they indicate that the Advanced Parallelism feature is functioning correctly. It's worth noting that in the PyTorch script we examined earlier, there was no handling for using multiple GPUs simultaneously.","You can confirm that the training is progressing smoothly by observing the loss values decreasing as follows.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 16 AMD MI250 GPUs: approximately 60,000 tokens/sec","Approximate training time based on GPU type and quantity is as follows:","When using 16 AMD MI250 GPUs: approximately 50 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["Running the train_mistral.py script, as in the previous section, will save the resulting model in the mistral_code_generation directory. This is a pure PyTorch model parameter file and is fully compatible with regular GPU servers, not just the MoAI Platform.","You can test the trained model using the inference_mistral.py script located in the tutorial directory of the GitHub repository you downloaded earlier. In this test, the prompt \"Create a function that takes a list of strings as input and joins them with spaces\" was used.","Run the code below.","Upon examining the output, you can confirm that the model has appropriately generated the function as per the prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer to the 3. Model fine-tuning","Please contact your infrastructure provider and choose one of the following options before proceeding. KT HAC AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_mistral.py script again without changing the batch size.","If the training proceeds normally, you should see the following logs:","Compared to the previous execution results when the number of GPUs was half, you can see that the learning is the same and the throughput has improved.","When using AMD MI250 GPU 16 → 32 : approximately 60,000 tokens/sec → 110,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["From this tutorial, we have seen how to fine-tune the Mistral 7B model on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while retaining your existing training code. Additionally, using the MoAI platform, you can easily configure the number of GPUs you need without any code changes. So please don’t hesitate to dive in and develop new models quickly and effortlessly with your data!"]},{"l":"Learn more","p":["MoAI Platform's Advanced Parallelization (AP)","Llama2 Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"GPT Fine-tuning","p":["This tutorial guides you on how to fine-tune GPT-based models open-sourced by Hugging Face on the MoAI Platform. Throughout this tutorial, you'll learn how to utilize an AMD GPU cluster with the MoAI Platform and explore the benefits of improved performance and automatic parallelization."]},{"l":"Overview","p":["GPT is a language model architecture that uses only the Transformer decoder structure. It was first introduced by OpenAI with GPT-1 in 2018. Since then, OpenAI has developed GPT-2, GPT-3, and GPT-4 models by increasing the dataset size and model parameters used for pre-training. Among them, the models that have been open-sourced are GPT-1 and GPT-2.","As the basic architecture of GPT is open-source, Hugging Face offers various GPT-based models beyond those developed by OpenAI.","In this tutorial, we'll use the MoAI Platform to fine-tune the Cerebras-GPT-13B model for the code generation task."]},{"l":"Before You Start","p":["Be sure to obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. You can either request and use a trial container of the MoAI Platform or sign up for the public cloud service running on the MoAI Platform.","Inquiries for MoAI Platform Trial Container: support@moreh.io","KT Cloud Hyperscale AI Computing","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Checking MoAI Accelerator","p":["To train sLLMs like the GPT model we'll be guiding you through in this tutorial, you need to select an appropriate size MoAI Accelerator. First, use the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Finetuning Model"]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.2.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Install Required Python Packages","p":["Execute the following command to pre-install third-party Python packages required for script execution:"]},{"l":"Downloading Training Script","p":["Run the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will use the train_gpt.py script located inside the tutorial directory."]},{"l":"Downloading Training Data","p":["Hugging Face provides not only model checkpoints but also various datasets that can be used for model fine-tuning.","In this tutorial, we will use the mlabonne/Evol-Instruct-Python-26k dataset. This dataset consists of Python code written in response to given prompt conditions.","To download the training data, we will use the prepare_gpt_dataset.py script located in the dataset directory to download the dataset available on Hugging Face and preprocess it for immediate use in fine-tuning training.","The preprocessed dataset is saved as gpt_dataset.pt.","The saved dataset can be loaded and used in code as follows."]}],[{"l":"2. Understanding training code","p":["Once you have prepared all the training data, let's take a look at the contents of the train_gpt.py script to execute the actual fine-tuning process. In this step, you'll notice that the MoAI Platform offers full compatibility with PyTorch, meaning that the training code is 100% identical to typical PyTorch code for Nvidia GPUs. Furthermore, you'll see how efficiently the MoAI Platform implements complex parallelization techniques beyond what's traditionally possible.","We highly recommend proceeding with the tutorial using the provided script as is. Afterward, feel free to customize the script to fine-tune the Cerebras-GPT-13B model or any other publicly available model in a different manner. If needed, refer to the MoAI Platform application guide ( LLM Fine-tuning Parameter Guide) provided by Moreh."]},{"l":"Training Code","p":["All code remains fully consistent with general PyTorch usage.","Firstly, import the required modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the preprocessed dataset saved during the preparation for 1. Prepare Fine-tuning and define the data loaders.","Subsequently, the training proceeds similarly to general AI model training with Pytorch.","As shown above, you can code in the same way as traditional PyTorch code on MoAI Platform."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","For enormous language models like Cerebras-GPT-13B used in this tutorial, it is inevitable to train them using multiple GPUs. In such cases, if you were to use frameworks other than the MoAI Platform, you would need to employ parallelization techniques like Data Parallel, Pipeline Parallel, or Tensor Parallel for training.","For instance, if a user wants to apply DDP in a typical PyTorch code, the following code snippet would need to be added. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","Experience optimal distributed parallel processing like no other framework can offer, thanks to MoAI Platform's Advanced Parallelization (AP), a feature that optimizes and automates parallelization in ways not found in other frameworks. With the AP feature, you can easily secure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism, typically required for training large-scale models, with just one simple line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting the Accelerator","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Do you remember checking MoAI Accelerator in the GPT Fine-tuning document? Now let's set up the accelerator needed for learning.","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 256GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity.","로 잘 변경된 것을 확인할 수 있습니다."]},{"l":"Training Execution","p":["Execute the train_gpt.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","The training loss decreases as follows, confirming normal training progress.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 16 AMD MI250 GPUs: approximately 6800 tokens/sec","Approximate training times based on GPU type and quantity are as follows:","When using 16 AMD MI250 GPUs: approximately 81 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["As in the previous chapter, when you run the train_gpt.py script, the resulting model will be saved in the code_generation directory. This is a pure PyTorch model parameter file and is fully compatible compatible not only with MoAI Platform but also with regular GPU servers.","You can test the trained model using the inference_gpt.py script located under the tutorial directory of the GitHub repository you downloaded beforehand.","Run the train script.","Upon inspecting the output, you can confirm that the model has generated an appropriate function based on the prompt content."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer again to the 3. Model fine-tuning document.","Please contact your infrastructure provider and choose one of the following options before proceeding. ( KT Hyperscale AI Computing (HAC) AI Accelerator Information)","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_gpt.py script again.","Since the available GPU memory has doubled, let's also change the batch size to 32 and run it.","If the training proceeds normally, the following log will be output.","Compared to the previous execution results when the number of GPUs was half, you can see that the learning is the same and the throughput has improved.","When using AMD MI250 GPU 16 → 32 : approximately 6800 tokens/sec → 13000 tokens/sec"]}],[{"l":"6. Conclusion","p":["So far, we have looked at the process of fine-tuning the GPT-based model from HuggingFace on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while maintaining existing training code. Additionally, if you use the MoAI platform, you can easily configure the number of GPUs you need without changing any code. Try developing new models quickly and effortlessly with your data."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization(AP)","Llama2 Fine-tuning","Mistral Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Qwen Fine-tuning","p":["This tutorial introduces an example of fine-tuning the open-source Qwen1.5 7B model on the MoAI Platform. Through this tutorial, you'll learn how to leverage the AMD GPU cluster using the MoAI Platform and explore the benefits of performance and automatic parallelization."]},{"l":"Overview","p":["The Qwen1.5 7B model is an open-source LLM released by Tongyi Qianwen(通义千问) in China. In this tutorial, we'll be performing a code generation task on the MoAI Platform, fine-tuning the Qwen1.5 7B model using the python_code_instruction_18k_alpaca dataset, which consists of system prompts, instructions for code generation, input values, and the code to be generated."]},{"l":"Before You Start","p":["Be sure to obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. You can either request and use a trial container of the MoAI Platform or sign up for the public cloud service running on the MoAI Platform.","Inquiries for MoAI Platform Trial Container: support@moreh.io","KT Cloud Hyperscale AI Computing","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Check MoAI Accelerator","p":["To train models like the Llama2 model outlined in this tutorial, you need to select an appropriate size MoAI Accelerator. Start by using the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Model fine-tuning"]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.2.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_qwen.py script located inside the tutorial directory."]},{"l":"Download Training Data","p":["To download the training data, we'll use the prepare_qwen_dataset.py script located in the dataset directory. When you run the code, it will download the cnn_dailymail dataset, preprocess it for training, and save it as qwen_dataset.pt file.","The preprocessed dataset is saved as qwen_dataset.pt.","You can then load the stored dataset in your code like this:"]}],[{"l":"2. Understanding training code","p":["If you've prepared all the training data, let's now take a look at the train_qwen.py script to actually run the fine-tuning process. This script is a standard PyTorch code that executes fine-tuning based on the implementation of the Qwen model available in the Hugging Face Transformers library.","In this step, you'll observe that MoAI Platform is fully compatible with PyTorch, and the training code is exactly the same as standard PyTorch code designed for NVIDIA GPUs. Moreover, you can also see how efficiently MoAI Platform can implement complex parallelization techniques.","We recommend starting by using the provided script to complete the tutorial as-is. Afterwards, feel free to modify the script as desired to fine-tune the Qwen1.5 7B model in different ways. Thanks to MoAI Platform's full compatibility with PyTorch, such modifications are possible. If needed, refer to the LLM Fine-tuning parameter guide for assistance."]},{"l":"Training Code","p":["All the code used during training is exactly the same as the standard method of using PyTorch.","Firstly, import the required modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the preprocessed dataset saved during the preparation for 1. Prepare Fine-tuning and define the data loaders.","Subsequently, the training proceeds similarly to general AI model training with Pytorch.","As shown above, you can code in the same way as traditional PyTorch code on MoAI Platform."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","To train massive language models like Qwen1.5 7B, which we use in this tutorial, it's inevitable to utilize multiple GPUs. If using different frameworks, you'll need to implement parallelization techniques such as Data Parallel, Pipeline Parallel, and Tensor Parallel to proceed with training.","For instance, if a user wants to apply DDP in a typical PyTorch code, the following code snippet would need to be added. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","Experience optimal distributed parallel processing like no other framework can offer, thanks to MoAI Platform's Advanced Parallelization (AP), a feature that optimizes and automates parallelization in ways not found in other frameworks. With the AP feature, you can easily secure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism, typically required for training large-scale models, with just one simple line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Do you remember checking MoAI Accelerator in the Mistral Fine-tuning (ENG) document? Now let's set up the accelerator needed for learning.","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 64GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity.","로 잘 변경된 것을 확인할 수 있습니다."]},{"l":"학습 실행","p":["주어진 train_qwen.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다. 중간에 파란색으로 표시된 부분을 보시면 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴본 PyTorch 스크립트에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","You can confirm that the training is progressing smoothly by observing the loss values appearing as follows.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 16 AMD MI250 GPUs: approximately 59,000 tokens/sec","Approximate training time based on GPU type and quantity is as follows:","When using 16 AMD MI250 GPUs: approximately 40 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["Running the train_qwen.py script, as in the previous chapter, will save the resulting model in the qwen_code_generation directory. This is a pure PyTorch model parameter file and is fully compatible with regular GPU servers, not just the MoAI Platform.","You can test the trained model using the inference_qwen.py script located under the tutorial directory in the GitHub repository you downloaded earlier. In the test, the prompt \"Given a list of strings, create a function that joins them with spaces\" was used.","Run the code below.","Upon examining the output, you can confirm that the model has appropriately generated the function as per the prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer to the 3. Model fine-tuning","Please contact your infrastructure provider and choose one of the following options before proceeding. KT Hyperscale AI Computing (HAC) AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_qwen.py script again without changing the batch size.","If the training proceeds normally, you should see the following logs:","Compared to the previous execution results when the number of GPUs was half, you can see that the learning is the same and the throughput has improved.","When using AMD MI250 GPU 16 → 32 : approximately 59,000 tokens/sec → 105,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["So far, we've explored the process of fine-tuning the Qwen1.5 7B model on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while keeping your existing training code intact. Additionally, using MoAI Platform, you can easily configure the required number of GPUs without any code changes. So please don’t hesitate to dive in and develop new models quickly and effortlessly with your data!","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Llama2 Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning"]}],[{"l":"Baichuan2 Fine-tuning","p":["The following tutorial will take you through the steps required to fine-tune Baichuan2 13B model with an example dataset, using the MoAI Platform. Through the tutorial, you'll learn how to utilize an AMD GPU cluster with MoAI Platform and discover the benefits of improved performance and automatic parallelization."]},{"l":"Overview","p":["Baichuan2 is a large-scale multilingual language model developed by Baichuan Intelligent Technology. This model offers configurations with 70 billion and 130 billion parameters trained on vast datasets consisting of 2.6 trillion tokens.","In this tutorial, we'll fine-tune the Baichuan2 13B model using the MoAI Platform with the Bitext-customer-support-llm-chatbot-training-dataset, a text-generation e-commerce dataset."]},{"l":"Before You Start","p":["Be sure to obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. You can either request and use a trial container of the MoAI Platform or sign up for the public cloud service running on the MoAI Platform.","Inquiries for MoAI Platform Trial Container: support@moreh.io","KT Cloud Hyperscale AI Computing","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Check MoAI Accelerator","p":["To train models like the Llama2 model outlined in this tutorial, you need to select an appropriate size MoAI Accelerator. Start by using the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Model fine-tuning."]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the MoAI version required to execute it. In the example above, it indicates that PyTorch version 1.13.1+cu116 is running with MoAI version 24.3.0 installed.","If you encounter a conda: command not found message, or if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform to create a conda environment. If the moreh version is not 24.3.0 but a different version, please execute the following code."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_baichuan2_13b.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download Training Data","p":["Execute the following command to install third-party Python packages required for script execution:","To download the training data for this tutorial, we'll use the prepare_baichuan_dataset.py script located inside the dataset directory. When you run the code, it will download the Bitext-custormer-support-llm-chatbot dataset, and preprocess it for training, and save it as baichuan_dataset.pt.","The preprocessed dataset is saved as baichuan_dataset.pt.","Then, You can load the stored dataset in your code like this:"]}],[{"l":"2. Understanding training code","p":["If you've prepared all the training data, let's now take a look at the contents of train_baichuan2_13b.py script for the actual fine-tuning process. In this step, you'll notice that MoAI Platform ensures full compatibility with PyTorch, confirming that the training code is identical to the typical PyTorch code for NVIDIA GPUs. Additionally, you'll explore how efficiently MoAI Platform implements complex parallelization techniques beyond this.","First and foremost, it's recommended to proceed with the tutorial using the provided script as is until the end. Afterwards, you can modify the script as you wish to fine-tune the Baichuan model in different ways. If needed, refer to the LLM Fine-tuning Parameter Guide."]},{"l":"Training Code","p":["All the code is exactly the same as when using PyTorch conventionally.","First, import the necessary modules from the transformers library.","Load the model configuration and checkpoint from HuggingFace.","Load the preprocessed dataset saved during the 1. Preparing for Fine-tuning and define the data loader.","Subsequent training proceeds just like any other model training with PyTorch.","As shown above, with MoAI Platform, you can use your existing PyTorch scripts without any modifications."]},{"l":"About Advanced Parallelism","p":["The training script used in this tutorial includes the following additional line of code, which performs automatic parallelization provided by MoAI Platform.","For huge language models like Baichuan2 13B, it's inevitable to train them using multiple GPUs. In such cases, if you're not using MoAI Platform, you'll need to introduce parallelization techniques like Data Parallel, Pipeline Parallel, and Tensor Parallelism.","For instance, if you want to apply DDP in your PyTorch code, you would need to add the following code snippet: ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","DDP can be relatively easy to apply, but implementing techniques like pipeline parallelism or tensor parallelism requires quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Furthermore, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","On the other hand, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","Experience the optimal automated distributed parallel processing that is only possible with MoAI Platform's Advanced Parallelization (AP) feature, unlike anything you've encountered in other frameworks. With AP, you can easily configure the optimal parameters and environment variables for pipeline parallelism and tensor parallelism, typically required for training large-scale models, with just a single line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Enter 8 to use .","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Remember we checked the MoAI Accelerator in the previous Baichuan2 Finetuning step? Now, let's set up the required accelerators for the actual training process.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 256GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_baichuan2_13b.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","You can confirm that the training is progressing smoothly by observing the loss values decreasing as follows.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 8 AMD MI250 GPUs: approximately 191605 tokens/sec","Approximate training time based on GPU type and quantity is as follows:","When using 8 AMD MI250 GPUs: approximately 30 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["Similar to the previous chapter, when you execute the train_baichuan2_13b.py script, the resulting model will be saved in the baichuan_code_generation directory. This model, stored as a pure PyTorch parameter file, is fully compatible not only with the MoAI Platform but also with regular GPU servers.","You can test the trained model using the inference_baichuan.py script located under the tutorial directory of the GitHub repository you downloaded earlier.","Run the code below.","Upon inspecting the output, you can verify that the model has generated appropriate responses to the prompts."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer to the 3. Model fine-tuning","Please contact your infrastructure provider and choose one of the following options before proceeding. KT Hyperscale AI Computing(HAC) AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_baichuan2_13b.py script again.","Since the available GPU memory has doubled, let's increase the batch size to 2048 and run the training.","f the training proceeds normally, you should see the following log:","Upon comparison with the results obtained when the number of GPUs was halved, you'll notice that the training progresses similarly, with an improvement in throughput.","When using AMD MI250 GPU 16 → 32 : approximately 198,000 tokens/sec → 370,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["So far, we've explored the process of fine-tuning the Baichuan2 13B model, which is publicly available on Hugging Face, using the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while keeping your existing training code intact. Moreover, with MoAI Platform, you can effortlessly configure the number of GPUs you need without changing any code. So please dive in and develop new models quickly and effortlessly with your data!","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization(AP)","Llama2 Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Supported Documents","p":["Moreh Toolkit Guide","Prepare Fine-tuning on MoAI Platform","Advanced Parallelization (AP)","LLM Fine-tuning Parameter guide","KT Hyperscale AI Computing (HAC) AI Accelerator Information"]}],[{"i":"advanced-parallelizationap","l":"Advanced Parallelization(AP)"},{"i":"what-is-advanced-parallelization","l":"What is Advanced Parallelization?","p":["The MoAI Platform's Advanced Parallelization (AP) is the automatic model optimization and distributed parallel processing feature. Typically, ML engineers go through numerous trial and error processes to optimize model parallelization during training of large-scale models. They experiment with various parallelization techniques, considering the memory size of the GPU in use, measure performance for different option combinations available in each technique, and determine optimized environment variables. This is a very laborious task that can take skilled machine learning developers from weeks to months.","With the AP feature of the MoAI Platform, complex parallel processing and model optimization tasks can be automatically performed with just a single line of code, dramatically reducing the time and effort involved in training.","Normalized model training throughput: experienced developer optimizations over one moth vs. using the AP feature on the MoAI Platform"]},{"i":"why-is-parallelization-crucial","l":"\uD83D\uDCA1 Why is Parallelization crucial?","p":["As a simple example, let's calculate how much GPU memory is required to train the Llama2 13B model.","The Llama2 13B model contains approximately 13 billion parameters. The memory size required to load the model, depending on the FP16 data format, is approximately 25GB. A minimum of 100-150GB of memory is required for training components such as the Optimizer and gradients. Therefore, training is impossible with just the memory capacity of a typical single GPU (80128GB). This is why GPU parallel processing is essential for model training.","For example, when using NVIDIA's FSDP (Fully Sharded Data Parallel) or DeepSpeed, developers must manually adjust various parallelization settings. In this case, the following parameters must be carefully adjusted:","Parameter Sharding: FSDP requires specifying how to shard model parameters across GPUs. Incorrect settings can result in suboptimal performance or memory overflow errors. Optimizer Stat Sharding: Both FSDP and DeepSpeed require sharding the optimizer state for efficient memory usage and communication overhead, which entails complex configurations. Activation Checkpoint: Activation checkpointing may need to be activated to save memory, balancing additional computation overhead for saving memory and recalculating activations during backpropagation.","Using the MoAI Platform's AP function allows users to save time and effort on complex parallelization tasks.","With a single line of code below, the platform automatically handles the complexities of parallelization operations to assure optimal performance.","As a result, when training large models, users can easily obtain optimal parameters and environment variable combinations for parallelization techniques such as Data Parallelism or Pipeline Parallelism.","Users can focus on the goal of model training rather than the time-consuming procedure of configuring parallelization settings."]}],[{"l":"How to use AP","p":["By default, AP(Advanced Parallelism) operates on a node-by-node basis. Therefore, a multi-GPU environment is required to use AP. Before proceeding with the AP feature, please review your current accelerator information using the guide below. For detailed information on accelerator sizes, refer to the KT Hyperscale AI Computing (HAC) Service Accelerator Model Information."]},{"l":"How to Apply AP","p":["There are two ways to apply the AP feature:","Add a single line of code.","실행 코드에 다음 한줄을 추가하여 AP 기능을 킬 수 있습니다. (이를 주석처리하면 끌 수 있습니다.)","환경 변수로 입력하기","다음과 같이 터미널 세션의 환경변수로 AP 기능을 킬 수 있습니다. ( 0으로 설정하면 끌 수 있습니다.)"]},{"l":"Example Usage","p":["If you have an environment with two or more nodes ready, you can now create training code to use the AP feature. In this guide, we'll set up code using the Llama2 model. Note that the Llama2 model requires community license agreement and Hugging Face token information. Please refer to 1. Fine-tuning Preparation to prepare the training code.","Once the training code is ready, configure the PyTorch environment before running the training on the MoAI Platform. The example below shows the PyTorch 1.13.1+cu116 version running on MoAI Platform version 24.2.0. For detailed instructions, refer to the 1. Fine-tuning Preparation tutorial.","Once the PyTorch environment is set up, fetch the training code from the GitHub repository.","Clone the quickstart repository and check the quickstart/ap-example directory. You'll find the text_summarization_for_ap.py file prepared by Moreh for testing the AP feature. Let's apply the AP feature using this code.","The training configuration for testing is as follows. We will proceed with testing based on this configuration.","Batch Size: 64","Sequence Length: 1024","MoAI Accelerator: 4xLarge"]},{"i":"enabling-the-ap-feature-ap-feature-on","l":"Enabling the AP Feature (AP Feature ON)","p":["프로그램의 main 함수 시작 지점에 AP 기능을 켜는 line이 있습니다. 다음과 같이 AP를 적용한 후 학습을 실행합니다.","At the beginning of the program's main function, there's a line to enable the AP feature. Apply AP and then run the training as shown below.","When the training starts, you will see logs like the following:","As shown, by adding just one line to enable the AP feature, complex distributed parallel processing is executed, and training progresses. Next, we'll explain the scenario users might encounter if they do not use the AP feature."]},{"i":"disabling-the-ap-feature-ap-feature-off","l":"Disabling the AP Feature (AP Feature OFF)","p":["Let's examine the situation when the AP feature is not used. To verify this, comment out the line that enables the AP feature at the beginning of the Python program's main function.","Then proceed with the training.","After the training completes, you will see logs as the following.","In the above logs, you can see the message RuntimeError: Error Code 4: OUT_OF_MEMORY, indicating an Out of Memory (OOM) error caused by trying to load data exceeding the VRAM of the 1 device chip, which is 64GB.","If you were using a framework other than MoAI Platform, you would experience such inconvenience. However, as a user of the MoAI Platform, you can easily solve the troublesome OOM problem by applying the AP feature with just one line, without spending a long time calculating and deliberating separate parallelization optimizations."]}],[{"l":"Advanced Settings for AP","p":["While simply adding torch.moreh.option.enable_advanced_parallelization() allows you to use the basic AP functionality, you can easily customize the parallelization feature according to your preferences using various variables provided by the MoAI Platform."]},{"l":"Customizing AP Configuration","p":["When using the AP feature as an API in a Python program, you can set additional arguments to restrict specific configurations.","Below are the configurable variables that can be inputted into the API, allowing users to optimize distributed parallelization according to their needs.","pipeline_parallel( bool, Default: true): Whether Pipeline Parallel( Gpipe) is applied","num_stages( str, int,*** default: ‘auto’): Maximum number of stages in Pipeline Parallelism.","num_micro_batches( str, int, Default: ‘auto’): Number of micro-batches in Pipeline Parallelism.","activation_recomputation( str, bool, Default: ‘auto’): Whether activation recomputation is applied","distribute_parameter( str, bool, Default: ‘auto’): Whether the feature of distributing param and grad to GPU is applied","mixed_precision( bool, Default: true): Whether bfloat16 is applied"]},{"l":"Environment Variables for Performance and Log Information of AP","p":["AP generates multiple candidate configurations and calculates costs based on them. The speed of this process and the available configurations may vary depending on the hardware resources used by the user.","default = 0","default = 16","default = 80","For example, the default setting limits the available memory to 80% of the actual GPU memory.","However, compile time may vary depending on the CPU usage and number of CPU cores.","If MOREH_ADVANCED_PARALLELIZATION_DETAILED_LOG=1, it will be printed to the console.","If MOREH_ADVANCED_PARALLELIZATION_DETAILED_LOG=2, it will be saved in the autoconfig_log.dump format.","If the waiting time during compilation is long, it is recommended to increase this value and retry.","MOREH_ADVANCED_PARALLELIZATION_DETAILED_LOG","MOREH_ADVANCED_PARALLELIZATION_MAX_PARALLEL_COMPILE_THREADS","MOREH_ADVANCED_PARALLELIZATION_MEMORY_USAGE_CORRECTION_RATIO","Provides additional information during Advanced Parallelization compilation.","Represents the available memory of the GPU used during compilation in Advanced Parallelization.","Specifies the number of threads used by the compiler during compilation.","Therefore, increasing this value may not necessarily improve compilation speed.","These environment variables can be configured as follows.","value type = int"]}],[{"i":"learn-more-about-advanced-parallelization-ap","l":"Learn More About Advanced Parallelization (AP)","p":["Let's take a closer look at the logs related to AP.","The MoAI Platform generates various optimized configurations for parallel processing to find the best optimization. The following log indicates that the Compiler Config Generator has set the number of candidate configurations for parallelization to 30.","Then, it generates an operation graph for each candidate.","From the above log, we can see that it took approximately 6.1 seconds to compile the configurations.","Next, it estimates the possible candidate configurations again.","Thus, it confirms that there are a total of 7 possible configurations.","Now, the graph simulator calculates the cost for each configuration, and once the calculation is complete, it selects the optimal configuration as the final choice.","The log above shows that it took about 0.8 seconds to calculate the cost until one final configuration was selected.","This information is recorded in a file named advanced_parallelization_selected_config.dump, which is created in the location where the Python program is executed. Now, let's see how advanced_parallelization_selected_config.dump looks like.","In this way, by adding just one line of code, it is possible to compute multiple parallelization candidates and achieve optimal parallelization."]}],[{"l":"Prepare Fine-tuning on MoAI Platform","p":["The MoAI Platform can be customized with various GPUs while maintaining a consistent user experience via a command line interface(CLI). This uniform access ensures all users interact with the system in the same way, making it more efficient and intuitive.","Similar to general AI training environments, the MoAI Platform supports Python-based programming. This document focuses on setting up and using a conda virtual environment as the standard configuration for AI training."]},{"l":"Setting up a Conda Environment","p":["To begin training, first create a conda environment:","Replace my-env with your desired environment name.","Activate the conda environment:","Install PyTorch. The MoAI Platform supports various PyTorch versions, allowing you to choose the one that fits your needs.","Use the moreh-smi command to check the version of the installed Moreh solution and the details of the MoAI Accelerator in use. The current MoAI Accelerator is For more information about the MoAI Accelerator, refer to the specifications.","For optimal parameters recommended for fine-tuning each model on the MoAI Platform, refer to the LLM Fine-tuning parameter guide","For detailed usage of the moreh toolkit, including moreh-smi and moreh-switch-model, please refer to the Using the MoAI Platform Toolkit"]}],[{"l":"Moreh Toolkit Guide","p":["The Moreh Toolkit is a command line tool designed to efficiently manage and monitor MoAI Accelerators on the MoAI Platform. With just three commands ( moreh-smi, moreh-switch-model, update-moreh), users can effectively manage MoAI Accelerators and easily update MoAI Platform."]},{"l":"Key Features","p":["The main features of the Moreh Toolkit are as follows:","Monitoring MoAI Accelerators:","Use the moreh-smi command to monitor memory usage and process status in real-time.","Switching AI Accelerators:","Use the moreh-switch-model command to change AI accelerators and execute processes to achieve optimal performance.","Updating and Rolling Back MoAI Platform:","Use the update-moreh command to update Moreh solutions to the latest version or roll back to a previous version if needed."]},{"i":"1-moai-accelerator-monitoring-moreh-smi","l":"1. MoAI Accelerator Monitoring: moreh-smi","p":["moreh-smi is a command-line tool that allows users to manage and monitor the MoAI Accelerator. You can run it in a conda environment where MoAI Platform PyTorch is installed.","If you are currently running a training session using the MoAI Accelerator, running moreh-smi in another terminal session will display the running process information as follows. You can also use moreh-smi to quickly identify your Job ID, allowing for faster support response from MoAI Platform in case of training or inference issues. In the example below, the Job ID is 976356."]},{"i":"utilizing-moai-accelerators-multi-accelerator-feature","l":"Utilizing MoAI Accelerator's Multi Accelerator Feature","p":["By default, if users do not configure anything, there will only be one MoAI Accelerator in a VM or container environment. With one MoAI Accelerator, only one process can run. However, there may be cases where you want to run multiple processes concurrently in the same environment, even with a single MoAI Accelerator. For example, you may want to run multiple training experiments concurrently by changing the same source code or hyperparameters. In such cases, you can create multiple MoAI Accelerators within a single token using moreh-smi, enabling multiple processes to run concurrently.","Let's explore adding, modifying, and removing MoAI Accelerators with the following examples."]},{"l":"Adding MoAI Accelerators","p":["First, let's add a MoAI Accelerator. When you enter the moreh-smi device --add command to use two or more MoAI Accelerators, you will see the following interface.","When you input the integer corresponding to the model you want to use from 1 to 13, a MoAI Accelerator corresponding to the entered device number will be created with the message \"Create device success.\" Within one environment, you can create a maximum of 5 AI accelerators. If you need to create more MoAI Accelerators, please contact your infrastructure administrator.","In the example below, let's add the 10th MoAI Accelerator:"]},{"i":"changing-the-default-moai-accelerator-moreh-smi-device---switch","l":"Changing the Default MoAI Accelerator: moreh-smi device --switch","p":["moreh-smi device --switch {Device_ID} is a command that allows you to change the default MoAI Accelerator.","It can be used as follows:","This means that the current default MoAI Accelerator has been changed to Accelerator 1."]},{"i":"removing-moai-accelerators-moreh-smi-device---rm","l":"Removing MoAI Accelerators: moreh-smi device --rm","p":["This time, let's try to remove a specific accelerator corresponding to the specified device ID with the command moreh-smi device --rm {Device_ID}.","The MoAI Accelerator with Device ID 1, , has been removed using the above command. To confirm, when you run moreh-smi again, you will notice that the device has been removed."]},{"l":"Other Various Options Utilization","p":["moreh-smi provides various other options. You can use the --help option to see what options are available.","moreh-smi -p- Monitor detailed hardware status of MoAI Accelerators.","moreh-smi -t- Check MoAI Accelerator token information.","If you encounter issues during training, such as tangled processes or difficulty terminating, causing messages like \"Process Running,\" use the moreh-smi --reset command."]},{"i":"changing-moai-accelerators-moreh-switch-model","l":"Changing MoAI Accelerators: moreh-switch-model","p":["moreh-switch-model is a tool that allows you to change the flavor (specifications) of the currently configured MoAI Accelerator. By changing the flavor of the MoAI Accelerator, you determine how much GPU memory to use.","It can be used as follows:","For example, if the result of the moreh-smi command is as follows, it means that the \"MoAI Platform AI Accelerator model currently set as the default is Accelerator 0, and this MoAI Accelerator is of type model.\"","The moreh-switch-model command displays the following prompt:","If you enter an integer corresponding to the model to be used from 1 to 13 (device number), the message \" The MoAI Platform AI Accelerator model is successfully switched to {model_id}. will be displayed, and the MoAI Accelerator corresponding to the entered device number will be changed.","Let's change the MoAI Accelerator to as follows:","You can continue with the change or exit the MoAI Accelerator change by typing q or Q.","After the change is complete, when you run moreh-smi again to confirm, you will see the following result:","The MoAI Accelerator previously set as the Small.64GB model has been changed to the Large.256GB model."]},{"i":"updating-moai-platform-update-moreh","l":"Updating MoAI Platform: update-moreh","p":["update-moreh is a command that allows you to create a new conda environment and install MoAI Platform on it, or update the version of MoAI Platform already installed in the conda environment. You can use update-moreh in the following situations:","If you have created a new conda environment and MoAI Platform Python packages need to be installed, you can easily install the latest version of MoAI Platform using the update-moreh command.","If you want to use the latest version of MoAI Platform even in an existing conda environment where MoAI Platform is already installed, you can update the currently used MoAI Platform to the latest version using the update-moreh command alone.","There may be cases where you need to install a specific version of the MoAI Platform. In such cases, you can use the --target option to specify the specific version you want to install.","If the MoAI Platform is not functioning properly due to issues such as dependency conflicts between other packages in the conda environment, you may need to reconstruct the conda environment. In such cases, you can use update-moreh to restore the MoAI Platform within the conda environment. In the latter case, you can use the --force option to reconstruct the environment. (Can be used with the —-target option)"]}],[{"l":"LLM Fine-tuning Parameter guide","p":["1,122,745 MiB","1,138,546 MiB","1,403,047 MiB","1,651,008 MiB","1,680,233 MiB","1,706,797 MiB","1,764,955 MiB","1,767,888 MiB","1,800,656 MiB","1024","109872","11,562","11m","121,013","125,180","128","1292886 MiB","12m","13286","1360m","13m","1403,2189","144,124","1467646 MiB","1489,3","15,890","15,972","154,12123","157,859","16","1600235 MiB","163,839","172395","17m","186,353","191605","194,282","2,146,115 MiB","2,645,347 MiB","2,800,000 MiB","2,845,656 MiB","2048","20m","22m","238,212","24,156","24.2.0","24.3.0","24.5.0","24m","256","25m","26,111","27B","28m","2xlarge","3,460,240 MiB","3013826 MiB","30m","3143781 MiB","3181616 MiB","32","32,371","32,563","34m","35m","36m","376,493","38m","400m","4096","40m","442,982 MiB","47,679","480m","4xlarge","50,782","51,353","512","543816 MiB","56,385","560,835 MiB","560m","58531","586m","59m","62,481","62,582","62,740","626,391 MiB","62m","63,893","638,460 MiB","64","65,565","6841","69,840","720m","749065 MiB","784,485 MiB","790,572 MiB","790454 MiB","7m","8,934","81m","843,375 MiB","858,128 MiB","866,656 MiB","872m","8m","8xlarge","92,623","93,165","962m","99,873","9m","Advanced Parallelism is applied","Baichuan2 13B","batch size","Cerebras GPT 13B","Keep in mind that the name of the MoAI Accelerator mentioned here might differ based on the CSP utilized by user.","Llama2 13B","Mistral 7B","MoAI Accelerator","MoAI Platform version","Model","Number of Tokens","Qwen1.5 7B","sequence length","This guide provides optimal parameters offered by the MoAI Platform and should be used solely as a helpful reference during the training.","throughput","Training Time","True","VRAM Usage"]}],[{"i":"kt-hyperscale-ai-computing-hac-ai-accelerator-information","l":"KT Hyperscale AI Computing (HAC) AI Accelerator Information","p":["…","1","12","12xLarge.6144GB","2","24","24xLarge.12288GB","2xLarge.1024GB","3","3xLarge.1536GB","4","48","48xLarge.24576GB","4xLarge.2048GB","6","6xLarge.3072GB","8","8xLarge.4096GB","Large.256GB","Medium.128GB","MI250 0.5","MI250 1","MI250 12","MI250 16","MI250 192","MI250 2","MI250 24","MI250 32","MI250 4","MI250 48","MI250 8","MI250 96","Model","Number of Nodes","Physical GPUs","Small.64GB","The current HAC service is powered by AMD MI250 GPUs. While performance may vary depending on the model/application, you can generally expect the AMD MI250 to deliver performance comparable to that of a single NVIDIA A100.","The official KT Cloud Manual","xLarge.512GB"]}]]